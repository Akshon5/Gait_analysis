{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Human Gait Analysis Using Computer Vision","text":"<p>This documentation presents the development of a classical computer vision\u2013based system for human gait analysis in rehabilitation contexts.</p> <p>The project explores structured video preprocessing, silhouette extraction, and motion analysis techniques to transform walking patterns into quantifiable metrics.</p> <p>Use the navigation panel to access:</p> <ul> <li>Introduction \u2013 Problem motivation and system overview  </li> <li>Video Preprocessing \u2013 Frame processing, enhancement, and silhouette extraction pipeline  </li> </ul>"},{"location":"01-introduction/","title":"Beginning Our Journey into Human Gait Analysis Using Computer Vision","text":"<p>In this Computer Vision Lab, my team and I began exploring real-world healthcare and rehabilitation challenges that could be addressed using classical computer vision techniques. Our objective was to identify a problem that was both practically relevant and technically feasible without relying heavily on deep learning frameworks.</p> <p>During our initial exploration, we considered several domains. Applications such as traffic monitoring, facial recognition, and disease detection were evaluated. However, many of these problems required large annotated datasets and complex deep learning architectures, which were beyond the intended scope of a classical computer vision\u2013based system.</p> <p>After careful consideration, we identified a significant healthcare challenge: human gait analysis for rehabilitation and mobility assessment.</p>"},{"location":"01-introduction/#understanding-the-problem","title":"Understanding the Problem","text":"<p>Human gait refers to the pattern of walking exhibited by an individual. In rehabilitation centres and physiotherapy clinics, gait assessment is often performed through manual observation. Clinicians visually evaluate posture, stride length, walking symmetry, and balance to assess recovery progress.</p> <p>While this method is valuable, it presents several limitations:</p> <ul> <li>Subjective interpretation  </li> <li>Inconsistency across sessions  </li> <li>Difficulty detecting subtle improvements  </li> <li>Lack of quantifiable data for long-term tracking  </li> </ul> <p>These limitations highlighted the need for a system capable of transforming walking patterns into measurable and objective data.</p>"},{"location":"01-introduction/#our-approach","title":"Our Approach","text":"<p>To address this problem, we propose a computer vision\u2013based gait analysis system that processes walking videos and extracts meaningful motion and shape information.</p> <p>Our system is designed to:</p> <ul> <li>Capture walking sequences using a fixed camera setup  </li> <li>Extract silhouettes from video frames  </li> <li>Analyse motion across time  </li> <li>Compute gait-related metrics such as stride length, walking speed, and step frequency  </li> <li>Classify gait patterns into normal or abnormal categories  </li> </ul> <p>Unlike deep learning-heavy solutions, our focus is on building an interpretable and structured pipeline using classical computer vision techniques. This ensures computational efficiency and makes the system suitable for controlled rehabilitation environments.</p>"},{"location":"01-introduction/#initial-challenges","title":"Initial Challenges","text":"<p>As we begin development, we recognise several technical challenges:</p> <ul> <li>Ensuring consistent lighting and background conditions  </li> <li>Accurately extracting clean silhouettes  </li> <li>Detecting subtle motion variations  </li> <li>Maintaining feature consistency across sessions  </li> <li>Designing a reliable classification strategy  </li> </ul> <p>Addressing these challenges will require careful experimentation and systematic evaluation.</p>"},{"location":"01-introduction/#moving-forward","title":"Moving Forward","text":"<p>This project represents more than a technical implementation. It is an opportunity to bridge computer vision with practical healthcare applications. By transforming visual walking patterns into quantitative insights, we aim to contribute toward more objective and data-driven rehabilitation assessment.</p> <p>In the upcoming sections, we will detail the system architecture, processing pipeline, and feature extraction strategies that form the foundation of our gait analysis framework.</p> <p>Stay tuned as we continue documenting our progress and technical development.</p>"},{"location":"02-video_preprocessing/","title":"Video Preprocessing for Gait Analysis (Dashboard Output)","text":"<p>In this blog, I explain the complete preprocessing pipeline used in my Human Gait Analysis project.</p> <p>Unlike a simple single-output system, my implementation generates four synchronized video outputs, displayed together in a dashboard format. Each video represents a different stage of preprocessing.</p> <p>This multi-view visualization helps in understanding how raw data transforms step-by-step into a clean silhouette.</p>"},{"location":"02-video_preprocessing/#the-four-video-outputs","title":"The Four Video Outputs","text":"<p>My system produces the following four outputs simultaneously:</p> <ol> <li>Original Video with ROI Detection (Green Boxes)</li> <li>Grayscale / Enhanced Frame</li> <li>Binary Silhouette (Black Background)</li> <li>Final Foreground Overlay (Red Mask on Original Frame)</li> </ol> <p>Each is explained below.</p>"},{"location":"02-video_preprocessing/#1-original-video-with-roi-detection-green-boxes","title":"1. Original Video with ROI Detection (Green Boxes)","text":"<p>This is the raw video frame with detected regions highlighted using green bounding boxes.</p>"},{"location":"02-video_preprocessing/#what-are-these-green-boxes","title":"What Are These Green Boxes?","text":"<p>The green boxes represent Region of Interest (ROI) detected using contour detection after background subtraction.</p>"},{"location":"02-video_preprocessing/#how-are-they-formed","title":"How Are They Formed?","text":"<ol> <li>Background subtraction identifies moving areas.</li> <li>Thresholding converts frame to binary.</li> <li>Contours are detected.</li> <li>The largest contour (human body) is selected.</li> <li>A bounding rectangle is drawn around it.</li> </ol> <p>This technique is called:</p> <p>Foreground Detection + Contour-Based Bounding Box Extraction</p>"},{"location":"02-video_preprocessing/#purpose","title":"Purpose","text":"<ul> <li>Identifies the walking subject</li> <li>Tracks movement</li> <li>Eliminates irrelevant background regions</li> </ul>"},{"location":"02-video_preprocessing/#2-grayscale-contrast-enhanced-frame","title":"2. Grayscale / Contrast Enhanced Frame","text":"<p>The second video shows the grayscale version of the frame.</p> <p>Sometimes Histogram Equalization is applied to improve contrast.</p>"},{"location":"02-video_preprocessing/#why-this-step","title":"Why This Step?","text":"<ul> <li>Reduces color complexity</li> <li>Focuses only on intensity values</li> <li>Improves background subtraction accuracy</li> <li>Makes silhouette detection more stable under lighting variations</li> </ul> <p>This step prepares the frame for reliable thresholding.</p>"},{"location":"02-video_preprocessing/#3-binary-silhouette-black-background","title":"3. Binary Silhouette (Black Background)","text":"<p>This is the most important preprocessing output.</p> <p>The frame is converted into a binary image:</p> <ul> <li>White \u2192 Human body</li> <li>Black \u2192 Background</li> </ul>"},{"location":"02-video_preprocessing/#how-is-it-generated","title":"How Is It Generated?","text":"<ol> <li>Background subtraction</li> <li>Thresholding</li> <li>Morphological Opening (remove noise)</li> <li>Morphological Closing (fill holes)</li> </ol> <p>Elliptical kernels are used to maintain natural human body shape.</p> <p>This produces a clean silhouette.</p>"},{"location":"02-video_preprocessing/#4-final-foreground-overlay-red-mask","title":"4. Final Foreground Overlay (Red Mask)","text":"<p>The fourth video overlays the detected foreground onto the original frame using a red mask.</p>"},{"location":"02-video_preprocessing/#why-this-is-useful","title":"Why This Is Useful","text":"<ul> <li>Visually verifies correct human detection</li> <li>Helps debug segmentation errors</li> <li>Shows alignment between original frame and extracted foreground</li> </ul> <p>This confirms that preprocessing is working correctly.</p>"},{"location":"02-video_preprocessing/#why-use-a-4-video-dashboard","title":"Why Use a 4-Video Dashboard?","text":"<p>Instead of showing only final silhouette, this dashboard:</p> <ul> <li>Demonstrates transparency in processing</li> <li>Helps debug errors</li> <li>Improves presentation clarity</li> <li>Makes viva explanation easier</li> <li>Shows full pipeline visually</li> </ul> <p>It proves that preprocessing is not random \u2014 it is systematic and step-by-step.</p>"},{"location":"02-video_preprocessing/#summary-of-techniques-used","title":"Summary of Techniques Used","text":"<p>The preprocessing pipeline uses classical Computer Vision techniques:</p> <ul> <li>Frame resizing</li> <li>Background subtraction</li> <li>Thresholding</li> <li>Contour detection</li> <li>Bounding box extraction</li> <li>Morphological operations</li> <li>Mask overlay</li> </ul> <p>No deep learning segmentation was used.</p> <p>This keeps the system lightweight, computationally efficient, and syllabus-aligned.</p>"},{"location":"02-video_preprocessing/#conclusion","title":"Conclusion","text":"<p>The four synchronized video outputs clearly demonstrate how raw walking footage transforms into a structured, clean silhouette ready for gait feature extraction.</p> <p>Preprocessing ensures:</p> <ul> <li>Noise reduction</li> <li>Accurate human isolation</li> <li>Stable feature extraction</li> <li>Improved gait recognition reliability</li> </ul> <p>In the next blog, I will explain how features are extracted from the final silhouette.</p>"}]}